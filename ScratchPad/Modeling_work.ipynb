{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.max_columns',60)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/Users/huiru/GADataScience/FinalProject/Data/clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#drop these two columns because not useful right now\n",
    "df.drop(['yr_born', 'yr_died'], axis=1, inplace=True)\n",
    "#drop outliers\n",
    "df = df.query('numlines < 500')\n",
    "df = df.query('enj_score < 1400')\n",
    "df = df.query('numstanzas < 100')\n",
    "df = df.query('avgline_stanza < 150')\n",
    "df = df.query('title_lesk_abs < 2.0')\n",
    "df = df.query('conjunction_ratio < 100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yrpub</th>\n",
       "      <th>numlines</th>\n",
       "      <th>numstanzas</th>\n",
       "      <th>avgline_stanza</th>\n",
       "      <th>enj_score</th>\n",
       "      <th>ttr</th>\n",
       "      <th>abstraction_score</th>\n",
       "      <th>lesk_abs_score</th>\n",
       "      <th>pronoun_score</th>\n",
       "      <th>title_lesk_abs</th>\n",
       "      <th>conjunction_ratio</th>\n",
       "      <th>nps_ratio</th>\n",
       "      <th>vps_ratio</th>\n",
       "      <th>aps_ratio</th>\n",
       "      <th>avg_nps_cscore</th>\n",
       "      <th>avg_vps_cscore</th>\n",
       "      <th>avg_aps_cscore</th>\n",
       "      <th>1_w_nps_fr</th>\n",
       "      <th>2_w_nps_fr</th>\n",
       "      <th>3_w_nps_fr</th>\n",
       "      <th>4_w_nps_fr</th>\n",
       "      <th>5_w_nps_fr</th>\n",
       "      <th>6_w_nps_fr</th>\n",
       "      <th>7_w_nps_fr</th>\n",
       "      <th>8_w_nps_fr</th>\n",
       "      <th>9_w_nps_fr</th>\n",
       "      <th>10_w_nps_fr</th>\n",
       "      <th>11_w_nps_fr</th>\n",
       "      <th>12_w_nps_fr</th>\n",
       "      <th>13_w_nps_fr</th>\n",
       "      <th>14_w_nps_fr</th>\n",
       "      <th>15_w_nps_fr</th>\n",
       "      <th>16_w_nps_fr</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1991.127451</td>\n",
       "      <td>31.945378</td>\n",
       "      <td>5.235994</td>\n",
       "      <td>14.899795</td>\n",
       "      <td>49.581985</td>\n",
       "      <td>0.688841</td>\n",
       "      <td>1.283921</td>\n",
       "      <td>1.149667</td>\n",
       "      <td>0.428676</td>\n",
       "      <td>1.240094</td>\n",
       "      <td>6.278567</td>\n",
       "      <td>0.521790</td>\n",
       "      <td>0.331054</td>\n",
       "      <td>0.147155</td>\n",
       "      <td>1.555614</td>\n",
       "      <td>1.998132</td>\n",
       "      <td>1.177705</td>\n",
       "      <td>0.605100</td>\n",
       "      <td>0.261746</td>\n",
       "      <td>0.114220</td>\n",
       "      <td>0.014292</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.509804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>29.659346</td>\n",
       "      <td>31.253510</td>\n",
       "      <td>8.369928</td>\n",
       "      <td>17.365631</td>\n",
       "      <td>125.862089</td>\n",
       "      <td>0.118392</td>\n",
       "      <td>0.158912</td>\n",
       "      <td>0.141538</td>\n",
       "      <td>0.732490</td>\n",
       "      <td>0.164475</td>\n",
       "      <td>9.960350</td>\n",
       "      <td>0.084558</td>\n",
       "      <td>0.086283</td>\n",
       "      <td>0.054893</td>\n",
       "      <td>0.216508</td>\n",
       "      <td>0.412874</td>\n",
       "      <td>0.194084</td>\n",
       "      <td>0.119607</td>\n",
       "      <td>0.100868</td>\n",
       "      <td>0.072419</td>\n",
       "      <td>0.024333</td>\n",
       "      <td>0.012282</td>\n",
       "      <td>0.007965</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0.003222</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.500079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1909.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1988.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.613102</td>\n",
       "      <td>1.177185</td>\n",
       "      <td>1.059349</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>1.136364</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.278207</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1.421637</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.540326</td>\n",
       "      <td>0.201376</td>\n",
       "      <td>0.071115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2004.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>11.797436</td>\n",
       "      <td>0.696057</td>\n",
       "      <td>1.280269</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>0.183502</td>\n",
       "      <td>1.240255</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.539841</td>\n",
       "      <td>1.968246</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>0.605662</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2011.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.764834</td>\n",
       "      <td>1.385397</td>\n",
       "      <td>1.234960</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.565360</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.179140</td>\n",
       "      <td>1.662990</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.678929</td>\n",
       "      <td>0.316678</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.022990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2015.000000</td>\n",
       "      <td>453.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>1336.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>3.476744</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             yrpub     numlines   numstanzas  avgline_stanza    enj_score  \\\n",
       "count  1428.000000  1428.000000  1428.000000     1428.000000  1428.000000   \n",
       "mean   1991.127451    31.945378     5.235994       14.899795    49.581985   \n",
       "std      29.659346    31.253510     8.369928       17.365631   125.862089   \n",
       "min    1909.000000     1.000000     1.000000        0.666667     1.125000   \n",
       "25%    1988.000000    15.000000     1.000000        3.833333     6.000000   \n",
       "50%    2004.000000    24.000000     2.000000        8.000000    11.797436   \n",
       "75%    2011.000000    38.000000     6.000000       20.000000    36.000000   \n",
       "max    2015.000000   453.000000    91.000000      146.000000  1336.000000   \n",
       "\n",
       "               ttr  abstraction_score  lesk_abs_score  pronoun_score  \\\n",
       "count  1428.000000        1428.000000     1428.000000    1428.000000   \n",
       "mean      0.688841           1.283921        1.149667       0.428676   \n",
       "std       0.118392           0.158912        0.141538       0.732490   \n",
       "min       0.071429           0.500000        0.500000       0.000000   \n",
       "25%       0.613102           1.177185        1.059349       0.068966   \n",
       "50%       0.696057           1.280269        1.150000       0.183502   \n",
       "75%       0.764834           1.385397        1.234960       0.400000   \n",
       "max       1.000000           2.000000        1.650000       6.000000   \n",
       "\n",
       "       title_lesk_abs  conjunction_ratio    nps_ratio    vps_ratio  \\\n",
       "count     1428.000000        1428.000000  1428.000000  1428.000000   \n",
       "mean         1.240094           6.278567     0.521790     0.331054   \n",
       "std          0.164475           9.960350     0.084558     0.086283   \n",
       "min          0.625000           0.000000     0.111111     0.000000   \n",
       "25%          1.136364           2.000000     0.472222     0.278207   \n",
       "50%          1.240255           3.000000     0.520833     0.333333   \n",
       "75%          1.333333           5.600000     0.565360     0.380952   \n",
       "max          1.800000          97.000000     1.000000     0.666667   \n",
       "\n",
       "         aps_ratio  avg_nps_cscore  avg_vps_cscore  avg_aps_cscore  \\\n",
       "count  1428.000000     1428.000000     1428.000000     1428.000000   \n",
       "mean      0.147155        1.555614        1.998132        1.177705   \n",
       "std       0.054893        0.216508        0.412874        0.194084   \n",
       "min       0.000000        1.000000        1.000000        1.000000   \n",
       "25%       0.111111        1.421637        1.750000        1.000000   \n",
       "50%       0.142857        1.539841        1.968246        1.142857   \n",
       "75%       0.179140        1.662990        2.200000        1.250000   \n",
       "max       0.363636        3.476744        6.500000        3.000000   \n",
       "\n",
       "        1_w_nps_fr   2_w_nps_fr   3_w_nps_fr   4_w_nps_fr   5_w_nps_fr  \\\n",
       "count  1428.000000  1428.000000  1428.000000  1428.000000  1428.000000   \n",
       "mean      0.605100     0.261746     0.114220     0.014292     0.002896   \n",
       "std       0.119607     0.100868     0.072419     0.024333     0.012282   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.540326     0.201376     0.071115     0.000000     0.000000   \n",
       "50%       0.605662     0.259259     0.108108     0.000000     0.000000   \n",
       "75%       0.678929     0.316678     0.151515     0.022990     0.000000   \n",
       "max       1.000000     1.000000     0.666667     0.200000     0.250000   \n",
       "\n",
       "        6_w_nps_fr   7_w_nps_fr   8_w_nps_fr   9_w_nps_fr  10_w_nps_fr  \\\n",
       "count  1428.000000  1428.000000  1428.000000  1428.000000  1428.000000   \n",
       "mean      0.000881     0.000325     0.000239     0.000070     0.000116   \n",
       "std       0.007965     0.003042     0.003222     0.001383     0.002113   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       0.200000     0.047619     0.076923     0.037037     0.052632   \n",
       "\n",
       "       11_w_nps_fr  12_w_nps_fr  13_w_nps_fr  14_w_nps_fr  15_w_nps_fr  \\\n",
       "count  1428.000000  1428.000000  1428.000000         1428  1428.000000   \n",
       "mean      0.000006     0.000078     0.000008            0     0.000016   \n",
       "std       0.000232     0.002940     0.000308            0     0.000601   \n",
       "min       0.000000     0.000000     0.000000            0     0.000000   \n",
       "25%       0.000000     0.000000     0.000000            0     0.000000   \n",
       "50%       0.000000     0.000000     0.000000            0     0.000000   \n",
       "75%       0.000000     0.000000     0.000000            0     0.000000   \n",
       "max       0.008772     0.111111     0.011628            0     0.022727   \n",
       "\n",
       "       16_w_nps_fr        label  \n",
       "count  1428.000000  1428.000000  \n",
       "mean      0.000006     0.509804  \n",
       "std       0.000232     0.500079  \n",
       "min       0.000000     0.000000  \n",
       "25%       0.000000     0.000000  \n",
       "50%       0.000000     1.000000  \n",
       "75%       0.000000     1.000000  \n",
       "max       0.008772     1.000000  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.876861257253\n",
      "3 0.609889998362\n",
      "4 0.490101040705\n",
      "5 0.425307691662\n",
      "6 0.452721109791\n",
      "7 0.452149412578\n",
      "8 0.458391446837\n",
      "9 0.398614832963\n",
      "10 0.444104160492\n",
      "11 0.353723293925\n"
     ]
    }
   ],
   "source": [
    "# metrics.silhouette_score(X, labels, metric='euclidean')\n",
    "X = df.ix[:, :-1]\n",
    "for k in xrange(2,12):\n",
    "    km = KMeans(n_clusters=k, random_state=1)\n",
    "    km.fit(X)\n",
    "    labels = km.labels_\n",
    "    print k, metrics.silhouette_score(X, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 1]\n",
      "[[  1.99066905e+03   3.20974212e+01   5.33309456e+00   1.46611082e+01\n",
      "    3.32799248e+01   6.87153617e-01   1.28491469e+00   1.14990609e+00\n",
      "    3.98173340e-01   1.23894729e+00   5.96568334e+00   5.21929078e-01\n",
      "    3.31274834e-01   1.46796088e-01   1.55081601e+00   1.99181698e+00\n",
      "    1.17675068e+00   6.07028123e-01   2.61457744e-01   1.13121812e-01\n",
      "    1.39461917e-02   2.83673634e-03   8.54914980e-04   2.83704306e-04\n",
      "    2.28524783e-04   5.57569560e-05   8.60030983e-05   6.28361735e-06\n",
      "    7.95924865e-05   8.32944626e-06   0.00000000e+00   2.71050543e-20\n",
      "    6.28361735e-06]\n",
      " [  2.01112500e+03   2.53125000e+01   1.00000000e+00   2.53125000e+01\n",
      "    7.60759375e+02   7.62450954e-01   1.24056237e+00   1.13925150e+00\n",
      "    1.75937500e+00   1.29013836e+00   1.99281250e+01   5.15742637e-01\n",
      "    3.21431061e-01   1.62826302e-01   1.76490549e+00   2.27360451e+00\n",
      "    1.21933079e+00   5.20993711e-01   2.74313947e-01   1.62126775e-01\n",
      "    2.93913307e-02   5.46286615e-03   2.02955178e-03   2.13068182e-03\n",
      "    7.10227273e-04   7.10227273e-04   1.42045455e-03  -5.08219768e-21\n",
      "   -1.35525272e-20  -3.38813179e-21   0.00000000e+00   7.10227273e-04\n",
      "   -5.08219768e-21]]\n"
     ]
    }
   ],
   "source": [
    "# km = KMeans(n_clusters=2,  init='random', n_init=1 , max_iter = 100, random_state=1)\n",
    "km = KMeans(n_clusters=2, init='k-means++', n_init=10 , max_iter = 300, random_state=1)\n",
    "Y_hat = km.fit(X).labels_\n",
    "centroids = km.cluster_centers_\n",
    "# centroids\n",
    "y = km.predict(X)\n",
    "print y\n",
    "# y\n",
    "# plt.scatter(X[:,0], X[:,1], c=Y_hat, alpha=0.4)\n",
    "centroids = km.cluster_centers_\n",
    "print centroids\n",
    "\n",
    "# plt.scatter(centroids[:,0], centroids[:,1],s=100,c=np.unique(Y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleandf.dropna(inplace = True)\n",
    "# X.columns\n",
    "# plt.jet()\n",
    "# plt.scatter(X.abstraction_score, X.pronoun_score, c=Y_hat, alpha=0.4)\n",
    "# centroids = km.cluster_centers_\n",
    "# plt.scatter(centroids[:,0], centroids[:,1],s=100,c=np.unique(Y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'yrpub', u'numlines', u'numstanzas', u'avgline_stanza', u'enj_score',\n",
       "       u'ttr', u'abstraction_score', u'lesk_abs_score', u'pronoun_score',\n",
       "       u'title_lesk_abs', u'conjunction_ratio', u'nps_ratio', u'vps_ratio',\n",
       "       u'aps_ratio', u'avg_nps_cscore', u'avg_vps_cscore', u'avg_aps_cscore',\n",
       "       u'1_w_nps_fr', u'2_w_nps_fr', u'3_w_nps_fr', u'4_w_nps_fr',\n",
       "       u'5_w_nps_fr', u'6_w_nps_fr', u'7_w_nps_fr', u'8_w_nps_fr',\n",
       "       u'9_w_nps_fr', u'10_w_nps_fr', u'11_w_nps_fr', u'12_w_nps_fr',\n",
       "       u'13_w_nps_fr', u'14_w_nps_fr', u'15_w_nps_fr', u'16_w_nps_fr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.label.value_counts()\n",
    "df.columns[:-1]\n",
    "\n",
    "# df.drop('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numerical_features = df.get(df.columns[:-1]) \n",
    "features_array = numerical_features\n",
    "target = df.label.values\n",
    "features_train, features_test, target_train, target_test = train_test_split(features_array, target, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Testing Results \n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "not contemp       0.68      0.55      0.61       217\n",
      "    contemp       0.62      0.74      0.67       212\n",
      "\n",
      "avg / total       0.65      0.64      0.64       429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train logistic regression, evaluate on test\n",
    "lr = LogisticRegression(C=1)\n",
    "lr.fit(features_train, target_train)\n",
    "    \n",
    "#clf = train_test(data_out)\n",
    "target_predicted = lr.predict(features_test)\n",
    "\n",
    "print(\"Train Testing Results \\n\\n\")\n",
    "\n",
    "print(classification_report(target_test, target_predicted,\n",
    "                         target_names=['not contemp', 'contemp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.32718093  0.67281907]\n",
      " [ 0.43789886  0.56210114]\n",
      " [ 0.28610729  0.71389271]\n",
      " [ 0.42856422  0.57143578]\n",
      " [ 0.32889039  0.67110961]]\n"
     ]
    }
   ],
   "source": [
    "proba_lr = lr.predict_proba(features_test)\n",
    "\n",
    "print proba_lr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def plot_roc_curve(target_test, target_predicted_proba, this_label):\n",
    "    fpr, tpr, thresholds = roc_curve(target_test, target_predicted_proba[:, 1])\n",
    "    \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, label= this_label + ', ROC Area = %0.3f' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "    plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "    plt.title('ROC')\n",
    "    plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111eb0990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_curve(target_test, proba_lr, \"ooo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(features_train, target_train)\n",
    "preds = nb.predict(features_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.536130536131\n",
      "[[166  51]\n",
      " [148  64]]\n"
     ]
    }
   ],
   "source": [
    "print metrics.accuracy_score(target_test, preds)\n",
    "print metrics.confusion_matrix(target_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Let's try running a decision tree classifier and seeing if that is better than logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featurecols = ['numlines','numstanzas' ,'avgline_stanza','enj_score','ttr',\n",
    "             'abstraction_score', 'lesk_abs_score', 'pronoun_score', 'title_lesk_abs', 'conjunction_ratio',\n",
    "             'nps_ratio','vps_ratio','aps_ratio','avg_nps_cscore','avg_vps_cscore','avg_aps_cscore',\n",
    "             '1_w_nps_fr','2_w_nps_fr','3_w_nps_fr','4_w_nps_fr','5_w_nps_fr','6_w_nps_fr','7_w_nps_fr',\n",
    "             '8_w_nps_fr','9_w_nps_fr','10_w_nps_fr','11_w_nps_fr','12_w_nps_fr','13_w_nps_fr', '14_w_nps_fr',\n",
    "             '15_w_nps_fr','16_w_nps_fr'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df[featurecols]\n",
    "y = df.label.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30,random_state=1)\n",
    " \n",
    "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "treeclf.fit(X_train, y_train) \n",
    "preds = treeclf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5337995338\n",
      "[[133  74]\n",
      " [126  96]]\n"
     ]
    }
   ],
   "source": [
    "print metrics.accuracy_score(y_test, preds)\n",
    "print metrics.confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>conjunction_ratio</td>\n",
       "      <td>0.240435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pronoun_score</td>\n",
       "      <td>0.179870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enj_score</td>\n",
       "      <td>0.142385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>aps_ratio</td>\n",
       "      <td>0.136939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>avg_nps_cscore</td>\n",
       "      <td>0.107043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>vps_ratio</td>\n",
       "      <td>0.101595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abstraction_score</td>\n",
       "      <td>0.091732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7_w_nps_fr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8_w_nps_fr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9_w_nps_fr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10_w_nps_fr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>numlines</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>11_w_nps_fr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5_w_nps_fr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12_w_nps_fr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13_w_nps_fr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>14_w_nps_fr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>15_w_nps_fr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6_w_nps_fr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1_w_nps_fr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4_w_nps_fr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3_w_nps_fr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2_w_nps_fr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>numstanzas</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>avg_aps_cscore</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>avg_vps_cscore</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nps_ratio</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>title_lesk_abs</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lesk_abs_score</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ttr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>avgline_stanza</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>16_w_nps_fr</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature  importance\n",
       "9   conjunction_ratio    0.240435\n",
       "7       pronoun_score    0.179870\n",
       "3           enj_score    0.142385\n",
       "12          aps_ratio    0.136939\n",
       "13     avg_nps_cscore    0.107043\n",
       "11          vps_ratio    0.101595\n",
       "5   abstraction_score    0.091732\n",
       "22         7_w_nps_fr    0.000000\n",
       "23         8_w_nps_fr    0.000000\n",
       "24         9_w_nps_fr    0.000000\n",
       "25        10_w_nps_fr    0.000000\n",
       "0            numlines    0.000000\n",
       "26        11_w_nps_fr    0.000000\n",
       "20         5_w_nps_fr    0.000000\n",
       "27        12_w_nps_fr    0.000000\n",
       "28        13_w_nps_fr    0.000000\n",
       "29        14_w_nps_fr    0.000000\n",
       "30        15_w_nps_fr    0.000000\n",
       "21         6_w_nps_fr    0.000000\n",
       "16         1_w_nps_fr    0.000000\n",
       "19         4_w_nps_fr    0.000000\n",
       "18         3_w_nps_fr    0.000000\n",
       "17         2_w_nps_fr    0.000000\n",
       "1          numstanzas    0.000000\n",
       "15     avg_aps_cscore    0.000000\n",
       "14     avg_vps_cscore    0.000000\n",
       "10          nps_ratio    0.000000\n",
       "8      title_lesk_abs    0.000000\n",
       "6      lesk_abs_score    0.000000\n",
       "4                 ttr    0.000000\n",
       "2      avgline_stanza    0.000000\n",
       "31        16_w_nps_fr    0.000000"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the feature importances\n",
    "pd.DataFrame({'feature':featurecols, 'importance':treeclf.feature_importances_}).sort('importance', ascending = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
